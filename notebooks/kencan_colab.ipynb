{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Kencan AI Assistant - Google Colab Deployment\n",
        "\n",
        "This notebook sets up Kencan on Google Colab with free GPU access.\n",
        "\n",
        "**Setup Steps:**\n",
        "1. Enable GPU: Runtime â†’ Change runtime type â†’ GPU\n",
        "2. Run all cells in order\n",
        "3. Copy the ngrok URL and configure your local agent\n",
        "4. Start making requests!"
      ],
      "metadata": {
        "id": "title"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install dependencies\n",
        "!pip install -q transformers torch accelerate flask flask-cors pyngrok requests"
      ],
      "metadata": {
        "id": "install"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import libraries\n",
        "import os\n",
        "import json\n",
        "from flask import Flask, request, jsonify\n",
        "from pyngrok import ngrok\n",
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
        "from threading import Thread\n",
        "\n",
        "print(f\"GPU Available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU Device: {torch.cuda.get_device_name(0)}\")"
      ],
      "metadata": {
        "id": "imports"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Configuration\n",
        "MODEL_NAME = \"microsoft/phi-2\"  # Free, fast model that works on Colab\n",
        "NGROK_AUTH_TOKEN = \"YOUR_NGROK_TOKEN\"  # Get free token from ngrok.com\n",
        "\n",
        "# Set your ngrok token\n",
        "if NGROK_AUTH_TOKEN != \"YOUR_NGROK_TOKEN\":\n",
        "    ngrok.set_auth_token(NGROK_AUTH_TOKEN)"
      ],
      "metadata": {
        "id": "config"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load model\n",
        "print(\"Loading model...\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, trust_remote_code=True)\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    MODEL_NAME,\n",
        "    torch_dtype=torch.float16,\n",
        "    device_map=\"auto\",\n",
        "    trust_remote_code=True\n",
        ")\n",
        "\n",
        "print(\"Model loaded successfully!\")"
      ],
      "metadata": {
        "id": "load_model"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create Flask API\n",
        "app = Flask(__name__)\n",
        "\n",
        "@app.route('/health', methods=['GET'])\n",
        "def health():\n",
        "    return jsonify({\"status\": \"healthy\", \"gpu\": torch.cuda.is_available()})\n",
        "\n",
        "@app.route('/command', methods=['POST'])\n",
        "def process_command():\n",
        "    data = request.json\n",
        "    user_input = data.get('input', '')\n",
        "    \n",
        "    # Generate response\n",
        "    prompt = f\"\"\"You are Kencan, an AI assistant that helps control a Windows PC.\n",
        "User request: {user_input}\n",
        "Provide a JSON response with 'action' and 'parameters' fields.\n",
        "Response:\"\"\"\n",
        "    \n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
        "    outputs = model.generate(\n",
        "        **inputs,\n",
        "        max_new_tokens=512,\n",
        "        temperature=0.7,\n",
        "        do_sample=True\n",
        "    )\n",
        "    response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    \n",
        "    return jsonify({\n",
        "        \"response\": response,\n",
        "        \"model\": MODEL_NAME\n",
        "    })\n",
        "\n",
        "@app.route('/finetune', methods=['POST'])\n",
        "def finetune():\n",
        "    # Endpoint for fine-tuning with custom data\n",
        "    return jsonify({\"message\": \"Fine-tuning endpoint - implement with your data\"})\n",
        "\n",
        "print(\"Flask app created!\")"
      ],
      "metadata": {
        "id": "flask_app"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Start server with ngrok\n",
        "port = 5000\n",
        "\n",
        "# Start ngrok tunnel\n",
        "public_url = ngrok.connect(port)\n",
        "print(f\"\\n{'='*60}\")\n",
        "print(f\"ðŸš€ Kencan AI Assistant is running!\")\n",
        "print(f\"ðŸ“¡ Public URL: {public_url}\")\n",
        "print(f\"{'='*60}\\n\")\n",
        "print(\"Copy the URL above and configure it in your local agent's config/settings.json\")\n",
        "print(\"\\nEndpoints:\")\n",
        "print(f\"  - Health check: {public_url}/health\")\n",
        "print(f\"  - Commands: {public_url}/command\")\n",
        "print(f\"  - Fine-tune: {public_url}/finetune\")\n",
        "\n",
        "# Run Flask app\n",
        "from flask import Flask\n",
        "from werkzeug.serving import run_simple\n",
        "\n",
        "run_simple('0.0.0.0', port, app, use_reloader=False, use_debugger=False)"
      ],
      "metadata": {
        "id": "start_server"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
