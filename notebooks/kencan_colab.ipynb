{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Kencan AI Assistant - Google Colab Deployment\n",
        "\n",
        "This notebook sets up Kencan on Google Colab with free GPU access.\n",
        "\n",
        "**Setup Steps:**\n",
        "1. Enable GPU: Runtime â†’ Change runtime type â†’ GPU\n",
        "2. Run all cells in order\n",
        "3. Copy the ngrok URL and configure your local agent\n",
        "4. Start making requests!"
      ],
      "metadata": {
        "id": "title"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Install dependencies\n",
        "!pip install -q transformers torch accelerate flask flask-cors pyngrok requests"
      ],
      "metadata": {
        "id": "install"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import libraries\n",
        "import os\n",
        "import json\n",
        "from flask import Flask, request, jsonify\n",
        "from pyngrok import ngrok\n",
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
        "from threading import Thread\n",
        "\n",
        "print(f\"GPU Available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU Device: {torch.cuda.get_device_name(0)}\")"
      ],
      "metadata": {
        "id": "imports"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Configuration\n",
        "MODEL_NAME = \"microsoft/phi-2\"  # Free, fast model that works on Colab\n",
        "NGROK_AUTH_TOKEN = \"YOUR_NGROK_TOKEN\"  # Get free token from ngrok.com\n",
        "\n",
        "# Set your ngrok token\n",
        "if NGROK_AUTH_TOKEN != \"YOUR_NGROK_TOKEN\":\n",
        "    ngrok.set_auth_token(NGROK_AUTH_TOKEN)"
      ],
      "metadata": {
        "id": "config"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load model\n",
        "print(\"Loading model...\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, trust_remote_code=True)\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    MODEL_NAME,\n",
        "    torch_dtype=torch.float16,\n",
        "    device_map=\"auto\",\n",
        "    trust_remote_code=True\n",
        ")\n",
        "\n",
        "print(\"Model loaded successfully!\")"
      ],
      "metadata": {
        "id": "load_model"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create Flask API with command queue and conversation memory\n",
        "import uuid\n",
        "from flask_cors import CORS\n",
        "\n",
        "app = Flask(__name__)\n",
        "CORS(app)\n",
        "\n",
        "# Command queue for pending commands\n",
        "command_queue = []\n",
        "command_results = {}\n",
        "\n",
        "# Conversation memory (last N exchanges)\n",
        "conversation_history = []\n",
        "MAX_HISTORY = 10\n",
        "\n",
        "@app.route('/health', methods=['GET'])\n",
        "def health():\n",
        "    return jsonify({\"status\": \"healthy\", \"gpu\": torch.cuda.is_available()})\n",
        "\n",
        "@app.route('/command', methods=['POST'])\n",
        "def process_command():\n",
        "    try:\n",
        "        data = request.json\n",
        "        if not data:\n",
        "            return jsonify({'success': False, 'error': 'No JSON data provided'}), 400\n",
        "        \n",
        "        user_input = data.get('input', '')\n",
        "        if not user_input:\n",
        "            return jsonify({'success': False, 'error': 'No input provided'}), 400\n",
        "        \n",
        "        # Build context from conversation history\n",
        "        history_context = ''\n",
        "        if conversation_history:\n",
        "            history_context = 'Previous conversation:\\n'\n",
        "            for h in conversation_history[-MAX_HISTORY:]:\n",
        "                history_context += f\"User: {h['user']}\\nAssistant: {h['assistant']}\\n\"\n",
        "            history_context += '\\n'\n",
        "        \n",
        "        # Generate response with context\n",
        "        prompt = f\"\"\"You are Kencan, an AI assistant that helps control a Windows PC.\n",
        "Available actions: open_browser, new_tab, close_tab, search_web, click_element, type_text,\n",
        "install_program, uninstall_program, run_command, open_application, create_file, read_file, delete_file, research.\n",
        "\n",
        "{history_context}Current request: {user_input}\n",
        "\n",
        "Respond with a JSON object containing 'action' and 'parameters' fields.\n",
        "Example: {{\"action\": \"search_web\", \"parameters\": {{\"query\": \"weather today\"}}}}\n",
        "Response:\"\"\"\n",
        "        \n",
        "        inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
        "        outputs = model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=512,\n",
        "            temperature=0.7,\n",
        "            do_sample=True,\n",
        "            pad_token_id=tokenizer.eos_token_id\n",
        "        )\n",
        "        response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "        \n",
        "        # Extract just the response part (after the prompt)\n",
        "        response = response.split('Response:')[-1].strip()\n",
        "        \n",
        "        # Save to conversation history\n",
        "        conversation_history.append({'user': user_input, 'assistant': response})\n",
        "        if len(conversation_history) > MAX_HISTORY * 2:\n",
        "            conversation_history.pop(0)\n",
        "        \n",
        "        # Create command with ID and add to queue\n",
        "        command_id = str(uuid.uuid4())\n",
        "        try:\n",
        "            # Try to parse as JSON command\n",
        "            import re\n",
        "            json_match = re.search(r'\\{.*\\}', response, re.DOTALL)\n",
        "            if json_match:\n",
        "                command_json = json.loads(json_match.group())\n",
        "                command_json['id'] = command_id\n",
        "                command_queue.append(command_json)\n",
        "        except json.JSONDecodeError:\n",
        "            pass  # Response wasn't valid JSON, that's ok\n",
        "        \n",
        "        return jsonify({\n",
        "            'success': True,\n",
        "            'response': response,\n",
        "            'command_id': command_id,\n",
        "            'model': MODEL_NAME\n",
        "        })\n",
        "    except Exception as e:\n",
        "        return jsonify({'success': False, 'error': str(e)}), 500\n",
        "\n",
        "@app.route('/commands/pending', methods=['GET'])\n",
        "def get_pending_commands():\n",
        "    \"\"\"Return pending commands and clear the queue\"\"\"\n",
        "    global command_queue\n",
        "    commands = command_queue.copy()\n",
        "    command_queue = []\n",
        "    return jsonify({'commands': commands})\n",
        "\n",
        "@app.route('/commands/<command_id>/result', methods=['POST'])\n",
        "def receive_command_result(command_id):\n",
        "    \"\"\"Receive execution result from local agent\"\"\"\n",
        "    try:\n",
        "        data = request.json\n",
        "        command_results[command_id] = data\n",
        "        return jsonify({'success': True, 'message': 'Result received'})\n",
        "    except Exception as e:\n",
        "        return jsonify({'success': False, 'error': str(e)}), 500\n",
        "\n",
        "@app.route('/conversation/clear', methods=['POST'])\n",
        "def clear_conversation():\n",
        "    \"\"\"Clear conversation history\"\"\"\n",
        "    global conversation_history\n",
        "    conversation_history = []\n",
        "    return jsonify({'success': True, 'message': 'Conversation cleared'})\n",
        "\n",
        "@app.route('/conversation/history', methods=['GET'])\n",
        "def get_conversation_history():\n",
        "    \"\"\"Get conversation history\"\"\"\n",
        "    return jsonify({'history': conversation_history})\n",
        "\n",
        "@app.route('/finetune', methods=['POST'])\n",
        "def finetune():\n",
        "    return jsonify({'message': 'Fine-tuning endpoint - implement with your data'})\n",
        "\n",
        "print('Flask app created with command queue and conversation memory!')"
      ],
      "metadata": {
        "id": "flask_app"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Start server with ngrok\n",
        "port = 5000\n",
        "\n",
        "# Start ngrok tunnel\n",
        "public_url = ngrok.connect(port)\n",
        "print(f\"\\n{'='*60}\")\n",
        "print(f\"ðŸš€ Kencan AI Assistant is running!\")\n",
        "print(f\"ðŸ“¡ Public URL: {public_url}\")\n",
        "print(f\"{'='*60}\\n\")\n",
        "print(\"Copy the URL above and configure it in your local agent's config/settings.json\")\n",
        "print(\"\\nEndpoints:\")\n",
        "print(f\"  - Health check: {public_url}/health\")\n",
        "print(f\"  - Send command: {public_url}/command\")\n",
        "print(f\"  - Pending commands: {public_url}/commands/pending\")\n",
        "print(f\"  - Command result: {public_url}/commands/<id>/result\")\n",
        "print(f\"  - Conversation history: {public_url}/conversation/history\")\n",
        "print(f\"  - Clear conversation: {public_url}/conversation/clear\")\n",
        "print(f\"  - Fine-tune: {public_url}/finetune\")\n",
        "\n",
        "# Run Flask app\n",
        "from flask import Flask\n",
        "from werkzeug.serving import run_simple\n",
        "\n",
        "run_simple('0.0.0.0', port, app, use_reloader=False, use_debugger=False)"
      ],
      "metadata": {
        "id": "start_server"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
